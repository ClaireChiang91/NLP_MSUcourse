{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_HW1_Part4_N-Gram language Models_v6",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMNrj8bexYkeR0LdDDitC5M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ClaireChiang91/NLP_MSUcourse/blob/main/NLP_HW1_Part4_N_Gram_language_Models_v6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psRUgYV49Z3T"
      },
      "source": [
        "This program is going to do with the assignment for the NLP course at MSU. \n",
        "https://github.com/deskool/nlp-class\n",
        "Homework1 Learning Excerse 4:\n",
        "https://github.com/deskool/nlp-class/blob/master/homework/HW1/assignment.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP8g31L6_7mm"
      },
      "source": [
        "**N-grams**\n",
        "An n-gram is a contiguous sequence of n items from a given sample of text or speech. Let's begin this section by defining a function that generates n-grams from our text data using the nltk library and using it to extract n-grams for n ranging from 1-5.\n",
        "**The challenge**\n",
        "Now that you have built your first language model, let's turn our attention back to the philosophers. For this final learning exercise, you will be building a tri-gram Nietzsche language model, and a tri-gram Russel Language Model using the 5 books from each of the philosophers you collected from Project Gutenburg earlier.\n",
        "\n",
        "Generate a conversation between your philosophers models by seeding one of the models with a tri-gram and allow it to run until it generates a terminating character (a '.', '?' or '!'). Then, have the other model generate a response by providing the ending tri-gram of the the first model as the seed tri-gram for the other model. Iterate like this for 15 sentences and print the text.\n",
        "\n",
        "Note: You probably learned from the earlier components of this assignment that Nietzsche and Russel have slightly different writing styles (yes, that was an understatement). For this reason, there are likely to be instances where the ending tri-gram from your Nietzsche model does not show up in the Russel language model and vice-versa. Use Laplace smoothing to handle this problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLXgF2SyoN1I",
        "outputId": "6712acdc-2c31-4a59-bfac-99153bc36c00"
      },
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "#to solve the error msg: Resource punkt not found. \n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l7v0puNHF_L"
      },
      "source": [
        "# Nietzsche\n",
        "website = 'http://www.gutenberg.org/cache/epub/4363/pg4363.txt'\n",
        "nietz    = requests.get(website).text[:200]\n",
        "\n",
        "# Russel\n",
        "website = 'http://www.gutenberg.org/cache/epub/5827/pg5827.txt'\n",
        "russel = requests.get(website).text[:200]\n",
        "########################################################################\n",
        "#to apply the request of iterating 15 sentences\n",
        "max_setence=15 \n",
        "#k smoothing, which could be 0.05, 0.5 or 1; 1 is to apply the reuqest of using Laplace\n",
        "k=1\n",
        "#stands for n-grams\n",
        "n=3"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vFxoS2rCbFw"
      },
      "source": [
        "#define an ngram extractor\n",
        "def extract_word_ngrams(data, num):\n",
        "    n_grams = ngrams(nltk.word_tokenize(data), num)\n",
        "    return [ ' '.join(grams) for grams in n_grams]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjP_axUMscax"
      },
      "source": [
        "def language_model(philosopher,size,k):\n",
        "  grams=extract_word_ngrams(philosopher, size)\n",
        "  vocabulary = list(set(grams))\n",
        "  counts = {}\n",
        "  for given_word in vocabulary:\n",
        "    counts[given_word] = {}\n",
        "    for next_word in vocabulary:\n",
        "        counts[given_word][next_word] = 0 \n",
        "\n",
        "  for i in range(len(grams)-1):\n",
        "    counts[grams[i]][grams[i+1]] += 1\n",
        "\n",
        "  # initialize the probabilites JSON object\n",
        "  probs = {}\n",
        "  for given_word in vocabulary:\n",
        "    probs[given_word] = {}\n",
        "    for next_word in vocabulary:\n",
        "      probs[given_word][next_word] = 0\n",
        "\n",
        "  # convert the counts to probabilites\n",
        "  for key, value in counts.items():\n",
        "    denominator = 0\n",
        "    for key2, value2 in counts[key].items():\n",
        "      denominator += value2\n",
        "    for key2, value2 in counts[key].items():\n",
        "      #Add-k smoothing\n",
        "      probs[key][key2] = (value2+k) / (denominator+k*len(vocabulary))\n",
        "  return probs"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6vPgZnkI-hu"
      },
      "source": [
        "def sample_next_gram_from_language_model(probs, given_token):\n",
        "    distribution            = list(probs[given_token].values())\n",
        "    sample_from_multinomial = np.random.multinomial(1,distribution)\n",
        "    sample_index            = np.where(sample_from_multinomial==1)[0][0]\n",
        "    word_keys               = list(probs[given_token].keys())\n",
        "    next_word               = word_keys[sample_index]\n",
        "    return(next_word)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoDfov_lJ0ru"
      },
      "source": [
        "def create_new_sentence(_probs_,seed_token):\n",
        "    tokens = [seed_token]\n",
        "    while any([' '.join(tokens).split(' ')[-1] not in ['.', '?','!'] ]):\n",
        "      _next_word_=sample_next_gram_from_language_model(_probs_,tokens[-1])\n",
        "      if all([_next_word_.split(' ')[i] not in ['.', '?','!'] for i in range(len(_next_word_.split(' ')))]):\n",
        "        tokens.append(_next_word_)\n",
        "      else:\n",
        "        i_index=0\n",
        "        while i_index < len(_next_word_.split(' ')):\n",
        "          _nextword_=_next_word_.split(' ')[i_index]\n",
        "          i_index+=1\n",
        "          if _nextword_ not in ['.', '?','!']:\n",
        "            tokens.append(_nextword_)\n",
        "          else:  \n",
        "            tokens.append(_nextword_)\n",
        "            break\n",
        "    return tokens"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "tBz3Ca7GfQ47",
        "outputId": "526dc823-4356-42b2-c619-935b9a0b922f"
      },
      "source": [
        "nietz_probs=language_model(nietz, n,k)\n",
        "seed_token_nietz= 'Good and Evil'\n",
        "' '.join(create_new_sentence(nietz_probs, seed_token_nietz))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Good and Evil cost and with This eBook is it , g no restrictions whatsoever Project Gutenberg EBook use of anyone is for the and with almost Beyond Good and Project Gutenberg EBook no restrictions whatsoever Beyond Good and Good and Evil EBook of Beyond at no cost of Beyond Good and with almost by Friedrich Nietzsche copy it , Friedrich Nietzsche This almost no restrictions anywhere at no Nietzsche This eBook the use of for the use at no cost may copy it may copy it .'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "-Icbf6oegpBh",
        "outputId": "eccb3fcd-80e7-4817-f447-b7e8cb88557d"
      },
      "source": [
        "russel_probs=language_model(russel, n,k)\n",
        "seed_token_russel= 'no cost and'\n",
        "\n",
        "' '.join(create_new_sentence(russel_probs, seed_token_russel))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"no cost and This eBook is Problems of Philosophy 's The Problems with almost no by Bertrand Russell Bertrand Russell This , by Bertrand Gutenberg 's The .\""
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MExA0voRgfcC"
      },
      "source": [
        "def conversation(seed_p,max_setence,_initialseed_):\n",
        "  initial_set=0\n",
        "  #opening\n",
        "  if initial_set==0:\n",
        "    if seed_p=='nietz':\n",
        "      #initial_seed='Good and Evil'\n",
        "      response=' '.join(create_new_sentence(nietz_probs, _initialseed_))\n",
        "      print(\"counts:\",initial_set)\n",
        "      print(seed_p,\"initially say:\",response)\n",
        "      print(\"=========\")  \n",
        "      initial_set+=1\n",
        "      seed_p='russel'\n",
        "    else:\n",
        "      #initial_seed='no cost and'\n",
        "      response=' '.join(create_new_sentence(russel_probs, _initialseed_))\n",
        "      print(\"counts:\",initial_set)\n",
        "      print(seed_p,\"initially say:\",response)\n",
        "      print(\"=========\")\n",
        "      initial_set+=1\n",
        "      seed_p='nietz'\n",
        "\n",
        "  #response\n",
        "  while initial_set<=max_setence:\n",
        "    next_seed=' '.join(response.split(\" \")[-3:])\n",
        "    if seed_p=='nietz':    \n",
        "      if next_seed in nietz_probs.keys():\n",
        "        response=' '.join(create_new_sentence(nietz_probs, next_seed))\n",
        "        print(\"counts:\",initial_set)\n",
        "        print(seed_p,\"responds:\",response)\n",
        "        print(\"=========\")\n",
        "        initial_set+=1\n",
        "        seed_p='russel'\n",
        "      else:\n",
        "        #starting for loop to look for the last three words in the dic\n",
        "        for i in range(0,len(response.split(\" \"))-3):\n",
        "          next_seed=' '.join(response.split(\" \")[(-4-i):(-1-i)])\n",
        "          if next_seed in nietz_probs.keys():\n",
        "            response=' '.join(create_new_sentence(nietz_probs, next_seed))\n",
        "            print(\"counts:\",initial_set)\n",
        "            print(seed_p,\"responds:\",response)\n",
        "            print(\"=========\")\n",
        "            initial_set+=1\n",
        "            seed_p='russel'\n",
        "            break\n",
        "          elif i==len(response.split(\" \"))-3-1 and next_seed not in nietz_probs.keys():\n",
        "            print(\"counts:\",initial_set)\n",
        "            print(seed_p,\"responds: no respone\")\n",
        "            print(\"===end======\")\n",
        "            initial_set+=1\n",
        "            seed_p='russel'\n",
        "\n",
        "    else: #the scenario of seed_p=='russel'\n",
        "      if next_seed in russel_probs.keys():\n",
        "        response=' '.join(create_new_sentence(russel_probs, next_seed))\n",
        "        print(\"counts:\",initial_set)\n",
        "        print(seed_p,\"responds:\",response)\n",
        "        print(\"=========\")\n",
        "        initial_set+=1\n",
        "        seed_p='nietz'\n",
        "      else:\n",
        "        for i in range(0,len(response.split(\" \"))-3):\n",
        "          next_seed=' '.join(response.split(\" \")[(-4-i):(-1-i)])\n",
        "          if next_seed in russel_probs.keys():\n",
        "            response=' '.join(create_new_sentence(russel_probs, next_seed))\n",
        "            print(\"counts:\",initial_set)\n",
        "            print(seed_p,\"responds:\",response)\n",
        "            print(\"=========\")\n",
        "            initial_set+=1\n",
        "            seed_p='nietz'\n",
        "            break\n",
        "          elif i==len(response.split(\" \"))-3-1 and next_seed not in russel_probs.keys():\n",
        "            print(\"counts:\",initial_set)\n",
        "            print(seed_p,\"responds: no respone\")\n",
        "            print(\"===end======\") \n",
        "            initial_set+=1\n",
        "            seed_p='nietz'"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhUqrL7Ijzju",
        "outputId": "45acd93f-1bf7-4971-a417-fa659bafd69b"
      },
      "source": [
        "conversation('nietz',max_setence,'Good and Evil' )"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "counts: 0\n",
            "nietz initially say: Good and Evil Nietzsche This eBook Project Gutenberg EBook Good and Evil restrictions whatsoever .\n",
            "=========\n",
            "counts: 1\n",
            "russel responds: restrictions whatsoever .\n",
            "=========\n",
            "counts: 2\n",
            "nietz responds: restrictions whatsoever .\n",
            "=========\n",
            "counts: 3\n",
            "russel responds: restrictions whatsoever .\n",
            "=========\n",
            "counts: 4\n",
            "nietz responds: restrictions whatsoever .\n",
            "=========\n",
            "counts: 5\n",
            "russel responds: restrictions whatsoever .\n",
            "=========\n",
            "counts: 6\n",
            "nietz responds: restrictions whatsoever .\n",
            "=========\n",
            "counts: 7\n",
            "russel responds: restrictions whatsoever .\n",
            "=========\n",
            "counts: 8\n",
            "nietz responds: restrictions whatsoever .\n",
            "=========\n",
            "counts: 9\n",
            "russel responds: restrictions whatsoever .\n",
            "=========\n",
            "counts: 10\n",
            "nietz responds: restrictions whatsoever .\n",
            "=========\n",
            "counts: 11\n",
            "russel responds: restrictions whatsoever .\n",
            "=========\n",
            "counts: 12\n",
            "nietz responds: restrictions whatsoever .\n",
            "=========\n",
            "counts: 13\n",
            "russel responds: restrictions whatsoever .\n",
            "=========\n",
            "counts: 14\n",
            "nietz responds: restrictions whatsoever .\n",
            "=========\n",
            "counts: 15\n",
            "russel responds: restrictions whatsoever .\n",
            "=========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7H6zrxSosDw7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}